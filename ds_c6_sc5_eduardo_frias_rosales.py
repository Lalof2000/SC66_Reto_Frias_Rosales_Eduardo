# -*- coding: utf-8 -*-
"""DS_C6_SC5_Eduardo_Frias_Rosales.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vatP9g8O58u6l8rE3TfatF4v3DHVmhkv

Reto: Analítica de texto aplicada al estudio de sentimientos para la toma de decisiones

Act:DS_C6_SC5

Eduardo Frias Rosales

Objetivo:

Construir un corpus utilizando las herramientas que has practicado a lo largo del módulo y realizar las siguientes actividades: calcular las frecuencias de longitudes de texto, frecuencias de palabras más comunes y extensión de vectores de stopwords.
Realizar las operaciones de procesamiento básico de textos: tokenizar, aplicar stemming, remover stopwords.
Crear gráficas para exploración de textos: histogramas, resumen de sentimientos, t-SNE o WordClouds.
Implementar y aplicar el análisis de sentimientos a los textos que se han recopilado en el corpus.
Instrucciones:

Descarga los archivos FinancialTweets.zip y Toys_and_Games.json, y realiza los siguientes pasos:

Elige uno de los datasetspara trabajar:
Análisis de sentimientos en un dataset de tweets financieros. Algunos de los influencers cuyos tweets fueron monitoreados son: 'MarketWatch', 'empresa', 'YahooFinance', 'TechCrunch', 'WSJ', 'Forbes', 'FT', 'TheEconomist', 'nytimes', 'Reuters', 'GerberKawasaki', 'jimcramer', ' TheStreet ',' TheStalwart ',' TruthGundlach ',' CarlCIcahn ',' ReformedBroker ',' benbernanke ',' bespokeinvest ',' BespokeCrypto ',' stlouisfed ',' federalreserve ',' GoldmanSachs ',' ianbremmer ',' MorganStanley ' , 'AswathDamodaran', 'mcuban', 'muddywatersre', 'StockTwits', 'SeanaNSmith'. Trabaja con el archivo FinancialTweets.zip.
Análisis de sentimientos de un dataset de reseñas de juguetes y juegos de Amazon. El archivo de origen se encuentra en formato .json. Por lo anterior, crear el corpus requiere de un procesamiento adicional de los documentos .json. Trabaja con el archivo Toys_and_Games.json.
Genera un archivo en Google Colab para el reto, cuya estructura esté basada en los análisis solicitados.
"""

# Elige uno de los datasetspara trabajar: Análisis de sentimientos de un dataset de reseñas de juguetes y juegos de Amazon. El archivo de origen se encuentra en formato .json. Por lo anterior, crear el corpus requiere de un procesamiento adicional de los documentos .json. Trabaja con el archivo Toys_and_Games.json.
# Unzip el archivo
#!unzip "/content/drive/MyDrive/Colab Notebooks/data/Analítica de texto/Toys_and_Games.zip" -d "/content/drive/MyDrive/Colab Notebooks/data/Analítica de texto/Toys_and_Games"

import pandas as pd
from google.colab import drive

import nltk
# Descargar recursos necesarios de NLTK
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
import matplotlib.pyplot as plt
from collections import Counter

# Carga la informacion JSON en un DataFrame
df = pd.read_json('/content/drive/MyDrive/Colab Notebooks/data/Analítica de texto/Toys_and_Games/Toys_and_Games.json', lines=True)
print(df.head())
df.info()

# Función para preprocesar el texto
def preprocesar_texto(texto):
  """
  Función para preprocesar el texto: tokenizar, aplicar stemming, remover stopwords.

  Args:
    texto: El texto a preprocesar.

  Returns:
    Una lista de tokens preprocesados.
  """
  # Tokenizar el texto
  tokens = word_tokenize(texto.lower())

  # Eliminar caracteres especiales y números
  tokens = [token for token in tokens if token.isalpha()]

  # Aplicar stemming
  stemmer = PorterStemmer()
  tokens = [stemmer.stem(token) for token in tokens]

  # Remover stopwords
  stop_words = set(stopwords.words('english'))
  tokens = [token for token in tokens if token not in stop_words]

  return tokens

# Corre la preparación y creación del corpus: cálculo de frecuencias de longitudes de texto, frecuencias de palabras más comunes, extensión de vectores de stopwords.

# Calcular las frecuencias de longitudes de texto
df['longitud_texto'] = df['reviewText'].apply(lambda x: len(x.split()))
frecuencias_longitud = df['longitud_texto'].value_counts()

# Calcular las frecuencias de palabras más comunes
todas_las_palabras = []
for texto in df['reviewText']:
  tokens = preprocesar_texto(texto)
  todas_las_palabras.extend(tokens)

frecuencias_palabras = Counter(todas_las_palabras)
palabras_mas_comunes = frecuencias_palabras.most_common(10)

# Extensión de vectores de stopwords
stop_words = set(stopwords.words('english'))
extension_stopwords = len(stop_words)

# Realiza las operaciones de procesamiento básico: tokenizar, aplicar stemming, remover stopwords.

# Aplicar la función de preprocesamiento a la columna 'reviewText'
df['tokens_preprocesados'] = df['reviewText'].apply(preprocesar_texto)

# Mostrar resultados
print("Frecuencias de longitudes de texto:\n", frecuencias_longitud)
print("\nFrecuencias de palabras más comunes:\n", palabras_mas_comunes)
print("\nExtensión de vectores de stopwords:", extension_stopwords)

# Mostrar algunos ejemplos de tokens preprocesados
print(df[['reviewText', 'tokens_preprocesados']].head())

# prompt: Crea gráficas para exploración de textos: histogramas, resumen de sentimientos, t-SNE o WordClouds.

# Histograma de longitudes de texto
plt.figure(figsize=(10, 6))
plt.hist(df['longitud_texto'], bins=50)
plt.xlabel('Longitud del texto')
plt.ylabel('Frecuencia')
plt.title('Histograma de longitudes de texto')
plt.show()

# Gráfico de barras para las palabras más comunes
top_words = [word for word, count in palabras_mas_comunes]
top_counts = [count for word, count in palabras_mas_comunes]

plt.figure(figsize=(10, 6))
plt.bar(top_words, top_counts)
plt.xlabel('Palabras')
plt.ylabel('Frecuencia')
plt.title('Palabras más comunes')
plt.xticks(rotation=45)
plt.show()

# Para WordClouds, necesitarías instalar la librería wordcloud:
# !pip install wordcloud
from wordcloud import WordCloud

# Crear una WordCloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(frecuencias_palabras)

# Mostrar la WordCloud
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()